{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TapasKumarDutta1/deep_neural_network_from_scratch_pneumonia_detection/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import os\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "from sklearn.metrics import *  # Import necessary metrics from scikit-learn\n",
        "\n",
        "np.random.seed(777)  # Set random seed for reproducibility\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # Ignore warnings to keep the output clean"
      ],
      "metadata": {
        "id": "mtCAlgl-JXWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(data_dir):\n",
        "    labels = [\"PNEUMONIA\", \"NORMAL\"]\n",
        "    data = []\n",
        "    for label in labels:\n",
        "        path = os.path.join(data_dir, label)\n",
        "        class_num = labels.index(label)\n",
        "        for en, img in enumerate(os.listdir(path)):\n",
        "            if en == 0:\n",
        "                print(img)\n",
        "            try:\n",
        "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
        "                resized_arr = cv2.resize(img_arr, (IMG_WIDTH, IMG_HEIGHT))\n",
        "                data.append([resized_arr, class_num])\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "    return np.array(data)\n",
        "\n",
        "# Load and preprocess training data\n",
        "train = get_data(\"../input/chest-xray-pneumonia/chest_xray/chest_xray/train\")\n",
        "\n",
        "trn = []\n",
        "tar = []\n",
        "for i in tqdm(range(5216)):\n",
        "    img = train[i][0]\n",
        "\n",
        "    trn.append(img.reshape((1, 240, 240, 1)))\n",
        "    tar.append(train[i][1])\n",
        "\n",
        "trn = np.asarray(trn)\n",
        "tar = np.asarray(tar)\n",
        "\n",
        "# Normalize training data\n",
        "trn = trn / 255\n"
      ],
      "metadata": {
        "id": "T_Doiuh1JY-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcitI7cDw-ny"
      },
      "source": [
        "# Function to shuffle two arrays in unison\n",
        "def unison_shuffled_copies(a, b):\n",
        "    \"\"\"\n",
        "    Shuffle two arrays in unison.\n",
        "\n",
        "    Parameters:\n",
        "        a (ndarray): First array to be shuffled.\n",
        "        b (ndarray): Second array to be shuffled.\n",
        "\n",
        "    Returns:\n",
        "        ndarray, ndarray: Shuffled versions of input arrays a and b.\n",
        "    \"\"\"\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "# Shuffle the training data and labels in unison\n",
        "trn, tar = unison_shuffled_copies(trn, tar)\n",
        "\n",
        "# Reshape training data and labels for further processing\n",
        "trn = trn.reshape((5216, 240, 240, 1))\n",
        "tar = tar.reshape((5216, 1))\n",
        "\n",
        "# LAYERS\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Sigmoid activation function.\n",
        "\n",
        "    Parameters:\n",
        "        x (ndarray): Input array.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Output array after applying sigmoid activation element-wise.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(x * -1))\n",
        "\n",
        "# Dot product (also known as inner product or scalar product) of two arrays\n",
        "def dot(a, b):\n",
        "    \"\"\"\n",
        "    Compute the dot product of two arrays.\n",
        "\n",
        "    Parameters:\n",
        "        a (ndarray): First input array.\n",
        "        b (ndarray): Second input array.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Result of the dot product.\n",
        "    \"\"\"\n",
        "    return np.sum(a * b, axis=(-1, -2, -3))\n",
        "\n",
        "# Glorot (Xavier) initialization for weight matrices\n",
        "def glorot(size1, in1, out1):\n",
        "    \"\"\"\n",
        "    Glorot (Xavier) initialization for weight matrices.\n",
        "\n",
        "    Parameters:\n",
        "        size1 (int): Size of the weight matrix.\n",
        "        in1 (int): Number of input units.\n",
        "        out1 (int): Number of output units.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Weight matrix with values initialized using the Glorot initialization method.\n",
        "    \"\"\"\n",
        "    dst = np.random.normal(0, 1, size1)\n",
        "    mx = np.max(dst)\n",
        "    mn = np.min(dst)\n",
        "\n",
        "    # Transform values to be within [0, 1]\n",
        "    dst = (dst - mn) / (mx - mn)\n",
        "\n",
        "    # Transform values to be within [-1, 1]\n",
        "    dst = (dst - 0.5) * 2\n",
        "\n",
        "    # Calculate the scale factor using the Glorot initialization formula\n",
        "    scl = 2.449489742783178 / (np.sqrt(in1 + out1))\n",
        "\n",
        "    # Scale the values within the range [-scl, scl]\n",
        "    dst *= scl\n",
        "    return dst\n",
        "\n",
        "\n",
        "# Maxpooling operation\n",
        "def maxpool(image, kernel):\n",
        "    \"\"\"\n",
        "    Maxpooling operation.\n",
        "\n",
        "    Parameters:\n",
        "        image (ndarray): Input image tensor of shape (batch_size, length, width, channels).\n",
        "        kernel (int): Size of the maxpooling kernel.\n",
        "\n",
        "    Returns:\n",
        "        ndarray, int, ndarray, ndarray: Maxpooled image tensor, kernel size, indices of max values, and input image tensor.\n",
        "    \"\"\"\n",
        "    bs, l, w, c = image.shape\n",
        "    out = np.zeros((bs, int(math.ceil(l / kernel)), int(math.ceil(w / kernel)), c))\n",
        "    idxs = []\n",
        "    for i in range(int(math.ceil(l / kernel))):\n",
        "        for j in range(int(math.ceil(w / kernel))):\n",
        "            look = image[:, i * kernel : (i + 1) * kernel, j * kernel : (j + 1) * kernel, :]\n",
        "            out[:, i, j, :] = np.max(look, axis=(1, 2))\n",
        "            idxs.append(np.where(np.in1d(look, out[:, i, j, :]))[0])\n",
        "    return out, kernel, np.asarray(idxs), image\n",
        "\n",
        "\n",
        "# Convolution operation\n",
        "def convolution(image, kernel, out):\n",
        "    \"\"\"\n",
        "    Convolution operation.\n",
        "\n",
        "    Parameters:\n",
        "        image (ndarray): Input image tensor of shape (batch_size, s1, s2, s3).\n",
        "        kernel (int): Size of the convolution kernel.\n",
        "        out (int): Number of output channels.\n",
        "\n",
        "    Returns:\n",
        "        ndarray, ndarray, ndarray: Convolutional weight matrix, input image tensor, and convolved output tensor.\n",
        "    \"\"\"\n",
        "    bs, s1, s2, s3 = image.shape\n",
        "    b = np.zeros((bs, (s1 - kernel) + 1, (s2 - kernel) + 1, out))\n",
        "    in1 = len(image.ravel()) / bs\n",
        "    out1 = len(b.ravel()) / bs\n",
        "    a = glorot((kernel, kernel, s3, out), kernel * kernel * s3, out)\n",
        "    for i in range(out):\n",
        "        for j in range(b.shape[0]):\n",
        "            for k in range(b.shape[1]):\n",
        "                out = dot(a[:, :, :, i], image[:, j : j + kernel, k : k + kernel])\n",
        "                b[:, j, k, i] = out\n",
        "    return a, image, b\n",
        "\n",
        "\n",
        "# Flatten operation\n",
        "def flatten(image):\n",
        "    \"\"\"\n",
        "    Flatten operation.\n",
        "\n",
        "    Parameters:\n",
        "        image (ndarray): Input image tensor.\n",
        "\n",
        "    Returns:\n",
        "        ndarray, ndarray: Input image tensor and flattened output tensor.\n",
        "    \"\"\"\n",
        "    bs, _, _, _ = image.shape\n",
        "    return image, image.reshape(bs, -1)\n",
        "\n",
        "\n",
        "# Dense layer\n",
        "def dense(input1, out):\n",
        "    \"\"\"\n",
        "    Dense layer.\n",
        "\n",
        "    Parameters:\n",
        "        input1 (ndarray): Input tensor of shape (batch_size, b).\n",
        "        out (int): Number of output units.\n",
        "\n",
        "    Returns:\n",
        "        ndarray, ndarray, ndarray: Dense layer weight matrix, input tensor, and output tensor.\n",
        "    \"\"\"\n",
        "    bs, b = input1.shape\n",
        "    in1 = b\n",
        "    out1 = out\n",
        "    weights = glorot((b, out), in1, out1)\n",
        "    out = np.dot(input1, weights)\n",
        "    return weights, input1, sigmoid(out)\n",
        "\n",
        "\n",
        "# Loss function\n",
        "def loss(true, pred):\n",
        "    \"\"\"\n",
        "    Cross-entropy loss function.\n",
        "\n",
        "    Parameters:\n",
        "        true (ndarray): True labels.\n",
        "        pred (ndarray): Predicted probabilities.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Loss value.\n",
        "    \"\"\"\n",
        "    return -1 * ((true * np.log(pred)) + ((1 - true) * (np.log(1 - pred))))\n",
        "\n",
        "\n",
        "# Loss derivative (backpropagation)\n",
        "def loss_back(true, pred):\n",
        "    \"\"\"\n",
        "    Derivative of the loss function.\n",
        "\n",
        "    Parameters:\n",
        "        true (ndarray): True labels.\n",
        "        pred (ndarray): Predicted probabilities.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Loss derivative.\n",
        "    \"\"\"\n",
        "    return pred - true\n",
        "\n",
        "\n",
        "# Rotate array by 180 degrees\n",
        "def rotate_180(array):\n",
        "    \"\"\"\n",
        "    Rotate the array by 180 degrees.\n",
        "\n",
        "    Parameters:\n",
        "        array (ndarray): Input array.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Rotated array.\n",
        "    \"\"\"\n",
        "    M, N = array.shape\n",
        "    out = np.zeros_like(array)\n",
        "    for i in range(M):\n",
        "        for j in range(N):\n",
        "            out[i, N - 1 - j] = array[M - 1 - i, j]\n",
        "    return out\n",
        "\n",
        "# Maxpooling derivative\n",
        "def maxpool_derivative(derivative, kernel, idxs):\n",
        "    \"\"\"\n",
        "    Maxpooling derivative.\n",
        "\n",
        "    Parameters:\n",
        "        derivative (ndarray): Derivative tensor of shape (batch_size, length, width, channels).\n",
        "        kernel (int): Size of the maxpooling kernel.\n",
        "        idxs (ndarray): Indices of max values.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Maxpooling derivative tensor of shape (batch_size, length * kernel, width * kernel, channels).\n",
        "    \"\"\"\n",
        "    bs, l, w, c = derivative.shape\n",
        "    out = np.zeros((bs, int(l * kernel), int(w * kernel), c))\n",
        "    en = 0\n",
        "    for i in range(l):\n",
        "        for j in range(w):\n",
        "            look = np.zeros((bs, kernel, kernel, c))\n",
        "            place = idxs[en]\n",
        "            look.ravel()[place] = 1\n",
        "            look *= derivative[:, i, j, :].reshape(bs, 1, 1, c)\n",
        "            en += 1\n",
        "            out[:, i * kernel : (i + 1) * kernel, j * kernel : (j + 1) * kernel, :] = look\n",
        "    return out\n",
        "\n",
        "\n",
        "# Calculate new gradients (backpropagation)\n",
        "def cng(grad, wts, x):\n",
        "    \"\"\"\n",
        "    Calculate new gradients during backpropagation for convolutional layers.\n",
        "\n",
        "    Parameters:\n",
        "        grad (ndarray): Gradient tensor of shape (batch_size, s1, s2, grad.shape[-1]).\n",
        "        wts (ndarray): Weights of the layer.\n",
        "        x (ndarray): Input tensor.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: New gradient tensor of the same shape as input tensor.\n",
        "    \"\"\"\n",
        "    bs, s1, s2, s3 = x.shape\n",
        "    grad1 = np.zeros((bs, s1 + 1, s2 + 1, grad.shape[-1]))\n",
        "    grad1[:, 1:s1, 1:s2, :] = grad\n",
        "    out = np.zeros_like(x)\n",
        "    p, q, r, s = wts.shape\n",
        "    for l in range(s):\n",
        "        for k in range(r):\n",
        "            for i in range(s2):\n",
        "                for j in range(s1):\n",
        "                    gd = dot(rotate_180(wts[:, :, k, l]), grad1[:, i : i + p, j : j + q, l])\n",
        "                    out[:, i, j, k] += gd\n",
        "    return out\n",
        "\n",
        "\n",
        "# Derivative of sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"\n",
        "    Derivative of the sigmoid function.\n",
        "\n",
        "    Parameters:\n",
        "        x (ndarray): Input tensor.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Derivative tensor.\n",
        "    \"\"\"\n",
        "    out = 1\n",
        "    return out\n",
        "\n",
        "\n",
        "# Convolution operation (only forward pass)\n",
        "def conv(input, kernel):\n",
        "    \"\"\"\n",
        "    Convolution operation (only forward pass).\n",
        "\n",
        "    Parameters:\n",
        "        input (ndarray): Input tensor of shape (batch_size, s1, s2, z).\n",
        "        kernel (ndarray): Convolution kernel of shape (_, q1, q2, x).\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Output tensor after convolution of shape (s1 - q1 + 1, s2 - q2 + 1, z, x).\n",
        "    \"\"\"\n",
        "    bs, s1, s2, z = input.shape\n",
        "    _, q1, q2, x = kernel.shape\n",
        "    b = np.zeros(((s1 - q1) + 1, (s2 - q2) + 1, z, x))\n",
        "    for l in range(z):\n",
        "        for y in range(x):\n",
        "            for j in range(b.shape[0]):\n",
        "                for k in range(b.shape[1]):\n",
        "                    out = dot(kernel[:, :, :, y], input[:, j : j + q1, k : k + q2, l]) / input.shape[0]\n",
        "                    b[j, k, l, y] = out\n",
        "    return b\n",
        "\n",
        "\n",
        "\n",
        "def calculate_grad(layers, true):\n",
        "    \"\"\"\n",
        "    Calculate gradients for each layer during backpropagation.\n",
        "\n",
        "    Parameters:\n",
        "        layers (list): List of layer dictionaries representing the neural network.\n",
        "        true (numpy.ndarray): The true labels of the training data.\n",
        "\n",
        "    Returns:\n",
        "        list: Updated list of layer dictionaries with computed gradients.\n",
        "\n",
        "    Notes:\n",
        "        This function performs backpropagation to compute gradients for each layer in the neural network.\n",
        "        The gradients are used to update the weights during training.\n",
        "\n",
        "    \"\"\"\n",
        "    for layer in layers:\n",
        "        # Extract relevant variables from the layer dictionary\n",
        "        try:\n",
        "            wts, inp, out, layer_type, pos = (\n",
        "                layer[\"weight\"],\n",
        "                layer[\"input\"],\n",
        "                layer[\"output\"],\n",
        "                layer[\"type\"],\n",
        "                layer[\"position\"],\n",
        "            )\n",
        "        except:\n",
        "            try:\n",
        "                out, shp, layer_type, pos = (\n",
        "                    layer[\"output\"],\n",
        "                    layer[\"shape\"],\n",
        "                    layer[\"type\"],\n",
        "                    layer[\"position\"],\n",
        "                )\n",
        "            except:\n",
        "                out, layer_type, pos, shp, index = (\n",
        "                    layer[\"output\"],\n",
        "                    layer[\"type\"],\n",
        "                    layer[\"position\"],\n",
        "                    layer[\"kernel\"],\n",
        "                    layer[\"index\"],\n",
        "                )\n",
        "\n",
        "        for i in range(0, pos):\n",
        "            # Handle different layer types and compute gradients accordingly\n",
        "            try:\n",
        "                wts1, inp1, out1, type1, pos1 = (\n",
        "                    layers[i][\"weight\"],\n",
        "                    layers[i][\"input\"],\n",
        "                    layers[i][\"output\"],\n",
        "                    layers[i][\"type\"],\n",
        "                    layers[i][\"position\"],\n",
        "                )\n",
        "            except:\n",
        "                try:\n",
        "                    out1, shp1, type1, pos1 = (\n",
        "                        layers[i][\"output\"],\n",
        "                        layers[i][\"shape\"],\n",
        "                        layers[i][\"type\"],\n",
        "                        layers[i][\"position\"],\n",
        "                    )\n",
        "                except:\n",
        "                    inp1, out1, type1, pos1, shp1, index1 = (\n",
        "                        layers[i][\"input\"],\n",
        "                        layers[i][\"output\"],\n",
        "                        layers[i][\"type\"],\n",
        "                        layers[i][\"position\"],\n",
        "                        layers[i][\"kernel\"],\n",
        "                        layers[i][\"index\"],\n",
        "                    )\n",
        "\n",
        "            if i == 0:\n",
        "                # Compute the initial gradient for the loss function\n",
        "                grad = loss_back(true, out1)\n",
        "                ot = sigmoid_derivative(out1)\n",
        "                grad *= ot\n",
        "                # grad.shape: (batch_size, 1)\n",
        "\n",
        "            if type1 == \"out\" and layer_type != \"out\" and i != pos - 1:\n",
        "                # Compute the gradient for previous hidden layers in fully connected networks\n",
        "                grad = np.dot(grad, wts1.T)\n",
        "                # grad.shape: (batch_size, -1)\n",
        "\n",
        "            if type1 == \"max\" and i != pos - 1:\n",
        "                # Compute the gradient for the maxpooling layer\n",
        "                grad = maxpool_derivative(grad, shp1, index1)\n",
        "\n",
        "            if type1 == \"flatten\" and i != pos - 1:\n",
        "                # Reshape the gradient to match the output shape of the flatten layer\n",
        "                grad = grad.reshape(shp1)\n",
        "\n",
        "            if type1 == \"conv\" and i != pos - 1:\n",
        "                # Compute the gradient for convolutional layers\n",
        "                grad = cng(grad, wts1, inp1)\n",
        "\n",
        "        if layer_type == \"out\" or layer_type == \"dense\":\n",
        "            # Compute the gradient for the fully connected or output layer\n",
        "            grad1 = np.dot(inp.T, grad) / true.shape[0]\n",
        "            layer[\"grad\"] = grad1\n",
        "\n",
        "        if layer_type == \"conv\":\n",
        "            # Compute the gradient for convolutional layers\n",
        "            grad1 = conv(inp, grad)\n",
        "            layer[\"grad\"] = grad1\n",
        "\n",
        "    return layers\n",
        "def propagate(layers, lr, itr):\n",
        "    \"\"\"\n",
        "    Update the weights of each layer based on gradients and momentum.\n",
        "\n",
        "    Parameters:\n",
        "        layers (list): List of layer dictionaries representing the neural network.\n",
        "        lr (float): Learning rate, controls the step size during weight updates.\n",
        "        itr (int): Current training iteration, used for momentum calculation.\n",
        "\n",
        "    Returns:\n",
        "        list: Updated list of layer dictionaries with updated weights.\n",
        "\n",
        "    Notes:\n",
        "        This function updates the weights of each layer based on the calculated gradients and momentum.\n",
        "        The learning rate (lr) and current training iteration (itr) are used to adjust the step size\n",
        "        during weight updates for faster or more stable convergence.\n",
        "\n",
        "    \"\"\"\n",
        "    for layer in layers:\n",
        "        if layer[\"type\"] != \"flatten\" and layer[\"type\"] != \"max\":\n",
        "            # Update the momentum for each layer using a moving average with momentum coefficient 0.9\n",
        "            layer[\"momentum\"] = 0.9 * layer[\"momentum\"] + 0.1 * layer[\"grad\"]\n",
        "\n",
        "            # Update the weights using the momentum and learning rate\n",
        "            layer[\"weight\"] -= lr * layer[\"momentum\"]\n",
        "\n",
        "    return layers\n",
        "def update(layers):\n",
        "    \"\"\"\n",
        "    Forward pass through the neural network to update the output of each layer.\n",
        "\n",
        "    Parameters:\n",
        "        layers (list): List of layer dictionaries representing the neural network.\n",
        "\n",
        "    Returns:\n",
        "        list: Updated list of layer dictionaries with updated outputs.\n",
        "\n",
        "    Notes:\n",
        "        This function performs a forward pass through the neural network to update the output of each layer.\n",
        "        The output of each layer is computed based on the input and weights. The function handles both\n",
        "        convolutional and output layers. The list of layer dictionaries is reversed at the beginning\n",
        "        to perform the forward pass in the correct order.\n",
        "\n",
        "    \"\"\"\n",
        "    layers = layers[::-1]  # Reverse the list to perform forward pass in the correct order\n",
        "    for en, layer in enumerate(layers):\n",
        "        try:\n",
        "            wts, inp, out, type, pos = (\n",
        "                layer[\"weight\"],\n",
        "                layer[\"input\"],\n",
        "                layer[\"output\"],\n",
        "                layer[\"type\"],\n",
        "                layer[\"position\"],\n",
        "            )\n",
        "        except:\n",
        "            try:\n",
        "                out, shp, type, pos = (\n",
        "                    layer[\"output\"],\n",
        "                    layer[\"shape\"],\n",
        "                    layer[\"type\"],\n",
        "                    layer[\"position\"],\n",
        "                )\n",
        "            except:\n",
        "                out, type, pos, shp, index = (\n",
        "                    layer[\"output\"],\n",
        "                    layer[\"type\"],\n",
        "                    layer[\"position\"],\n",
        "                    layer[\"kernel\"],\n",
        "                    layer[\"index\"],\n",
        "                )\n",
        "\n",
        "        # Perform forward pass for convolutional layer\n",
        "        if type == \"conv\":\n",
        "            bs, a, b, c = inp.shape\n",
        "            bs, x, y, z = out.shape\n",
        "            kernel = wts.shape[1]\n",
        "            for i in range(z):\n",
        "                for j in range(x):\n",
        "                    for k in range(y):\n",
        "                        out[:, j, k, i] = dot(\n",
        "                            wts[:, :, :, i], inp[:, j : j + kernel, k : k + kernel]\n",
        "                        )\n",
        "\n",
        "        # Perform forward pass for output layer\n",
        "        if type == \"out\":\n",
        "            out = np.dot(inp, wts)\n",
        "            out = sigmoid(out)\n",
        "\n",
        "        # Update the input of the next layer if it exists\n",
        "        if pos != 1:\n",
        "            layers[en + 1][\"input\"] = out\n",
        "\n",
        "        layer[\"output\"] = out  # Update the output of the current layer\n",
        "\n",
        "    return layers[::-1]  # Return the list of layer dictionaries in the original order\n",
        "\n",
        "\n",
        "def save(layers, epoch):\n",
        "    \"\"\"\n",
        "    Save the weights and momentum of each layer at a specific epoch.\n",
        "\n",
        "    Parameters:\n",
        "        layers (list): List of layer dictionaries representing the neural network.\n",
        "        epoch (int): Current epoch number.\n",
        "\n",
        "    Notes:\n",
        "        This function saves the weights and momentum of each layer at a specific epoch.\n",
        "        The saved files are in numpy format with names based on the layer type, index, and epoch number.\n",
        "\n",
        "    \"\"\"\n",
        "    for en, i in enumerate(layers):\n",
        "        try:\n",
        "            np.save(i[\"type\"] + str(en) + str(epoch) + \"_weight.npy\", i[\"weight\"])\n",
        "            np.save(i[\"type\"] + str(en) + str(epoch) + \"_momentum.npy\", i[\"momentum\"])\n",
        "        except:\n",
        "            continue\n",
        "def train(lr, trn, target, epoch):\n",
        "    \"\"\"\n",
        "    Train the neural network.\n",
        "\n",
        "    Parameters:\n",
        "        lr (float): Learning rate for training.\n",
        "        trn (numpy.ndarray): Training data.\n",
        "        target (numpy.ndarray): Target labels for training data.\n",
        "        epoch (int): Number of epochs for training.\n",
        "\n",
        "    Returns:\n",
        "        list: List of loss values for each epoch.\n",
        "        list: List of updated layer dictionaries representing the neural network.\n",
        "\n",
        "    Notes:\n",
        "        This function trains the neural network using stochastic gradient descent (SGD) for the specified number of epochs.\n",
        "        It calculates the loss for each mini-batch, updates the weights of the layers, and saves the weights at each epoch.\n",
        "        The function returns a list of loss values for each epoch and the final updated layer dictionaries.\n",
        "\n",
        "    \"\"\"\n",
        "    loss1 = []  # List to store loss values for each epoch\n",
        "    acc = []   # List to store accuracy values (not used in the provided code)\n",
        "    prev_loss = 999  # Variable to store previous loss (not used in the provided code)\n",
        "\n",
        "    for i in range(epoch):\n",
        "        LOSS1 = []  # List to store loss values for each mini-batch in the current epoch\n",
        "        for e in tqdm(range(int(math.ceil(trn.shape[0] / 32)))):\n",
        "            # Extract the mini-batch of data and corresponding labels\n",
        "            img = []\n",
        "            tar = []\n",
        "            for ii in range(e * 32, (e + 1) * 32):\n",
        "                if ii < trn.shape[0]:\n",
        "                    img.append(trn[ii].reshape(240, 240, 1))\n",
        "                    tar.append(target[ii])\n",
        "\n",
        "            img = np.asarray(img)\n",
        "            true = np.asarray(tar)\n",
        "\n",
        "            # Initialize or update the layers at the beginning of training\n",
        "            if e == 0 and i == 0:\n",
        "                a = convolution(img.reshape(32, 240, 240, 1), 2, 2)\n",
        "                b = convolution(a[2], 2, 4)\n",
        "                g = maxpool(b[2], 2)\n",
        "                f = flatten(g[0])\n",
        "                o = dense(f[1], 1)\n",
        "\n",
        "            layers = [\n",
        "                {\n",
        "                    \"weight\": o[0],\n",
        "                    \"input\": o[1],\n",
        "                    \"output\": o[2],\n",
        "                    \"type\": \"out\",\n",
        "                    \"position\": 1,\n",
        "                    \"momentum\": np.zeros_like(o[0]),\n",
        "                },\n",
        "                {\"shape\": f[0].shape, \"output\": f[1], \"type\": \"flatten\", \"position\": 2},\n",
        "                {\n",
        "                    \"output\": g[0],\n",
        "                    \"kernel\": g[1],\n",
        "                    \"index\": g[2],\n",
        "                    \"type\": \"max\",\n",
        "                    \"position\": 3,\n",
        "                    \"input\": g[-1],\n",
        "                },\n",
        "                {\n",
        "                    \"weight\": b[0],\n",
        "                    \"input\": b[1],\n",
        "                    \"output\": b[2],\n",
        "                    \"type\": \"conv\",\n",
        "                    \"position\": 4,\n",
        "                    \"momentum\": np.zeros_like(b[0]),\n",
        "                },\n",
        "                {\n",
        "                    \"weight\": a[0],\n",
        "                    \"input\": img,\n",
        "                    \"output\": a[2],\n",
        "                    \"type\": \"conv\",\n",
        "                    \"position\": 5,\n",
        "                    \"momentum\": np.zeros_like(a[0]),\n",
        "                },\n",
        "            ]\n",
        "\n",
        "            layers = calculate_grad(layers, true)\n",
        "            # Visualize gradients for debugging (optional)\n",
        "            for layer in layers:\n",
        "                try:\n",
        "                    sns.distplot(layer[\"grad\"].ravel())\n",
        "                    plt.show()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            layers = propagate(layers, lr, e)\n",
        "            layers = update(layers)\n",
        "            pred = layers[0][\"output\"]\n",
        "            LOSS = np.mean(loss(true, pred))\n",
        "            LOSS1.append(LOSS)\n",
        "            del [img, true, tar]\n",
        "            gc.collect()\n",
        "            save(layers, i)\n",
        "\n",
        "        print(np.mean(LOSS1))\n",
        "        loss1.append(np.mean(LOSS))\n",
        "    return loss1, layers\n",
        "\n",
        "\n",
        "loss1, layers = train(1e-4, trn, tar, 8)\n",
        "\n",
        "\n",
        "sns.scatterplot(x=range(len(loss1)), y=loss1)\n",
        "\n",
        "np.save(\"loss.npy\", loss1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnCdtIqWxABp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}