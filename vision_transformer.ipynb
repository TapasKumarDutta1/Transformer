{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM63GxLpKOQH6CZl16AH2Mv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TapasKumarDutta1/Transformer/blob/main/vision_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "LtNqmAKsM3fx"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, patch_size, in_channels):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_channels, in_channels, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # (B, E, H, W)\n",
        "        B,C,H,W = x.shape\n",
        "        x = x.view(B,C,H*W)  # (B, E, H*W)\n",
        "        x = x.transpose(1, 2)  # (B, H*W, E)\n",
        "        return x\n",
        "\n",
        "class Concatenate_CLS_Token(nn.Module):\n",
        "    def __init__(self, N, embed_dim):\n",
        "        super().__init__()\n",
        "        self.CLS = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "    def forward(self, x):\n",
        "        BATCH, _, _ =x.shape\n",
        "        CLS = self.CLS.repeat(BATCH,1,1)\n",
        "        x   = torch.concatenate([CLS, x],1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Add_Positional_Embedding(nn.Module):\n",
        "    def __init__(self, N, embed_dim):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.positional = nn.Parameter(torch.zeros(1, 1, int(embed_dim)))\n",
        "    def forward(self, x):\n",
        "        BATCH, _, _ =x.shape\n",
        "        positional = self.positional.repeat(BATCH,1,1)\n",
        "        x = x+positional\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,head,in_dim,dim):\n",
        "    super().__init__()\n",
        "    self.head = head\n",
        "    self.dim  = dim\n",
        "    self.query_projection = nn.Linear(in_dim, dim*self.head)\n",
        "    self.key_projection   = nn.Linear(in_dim, dim*self.head)\n",
        "    self.value_projection = nn.Linear(in_dim, dim*self.head)\n",
        "\n",
        "    self.out = nn.Linear(dim*self.head, in_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    query = self.query_projection(x)   #B, N, D*H\n",
        "    key   = self.key_projection(x)\n",
        "    value = self.value_projection(x)\n",
        "\n",
        "\n",
        "    B,N,D = x.shape\n",
        "\n",
        "    query = query.view(B, N, self.head, self.dim)  # B, N, H, D\n",
        "    key   = key.view(B, N, self.head, self.dim)\n",
        "    value = value.view(B, N, self.head, self.dim)\n",
        "\n",
        "\n",
        "    query = query.transpose(1,2)  # B, H, N, D\n",
        "    key   = key.transpose(1,2)\n",
        "    value = value.transpose(1,2)  # B, H, N, D\n",
        "\n",
        "\n",
        "    attention_map = torch.matmul(query, key.transpose(-1,-2))   #B, H, N, N\n",
        "    scaled_attention_map = attention_map / torch.sqrt(torch.tensor((self.dim)))\n",
        "    scaled_attention_map = torch.nn.Softmax(-1)(scaled_attention_map)\n",
        "\n",
        "\n",
        "    output = torch.matmul(scaled_attention_map, value)   # B, H, N, D\n",
        "    output = output.transpose(1,2)  # B, N, H, D\n",
        "    output = output.reshape(B, N, self.head*self.dim)\n",
        "    return self.out(output)\n",
        "\n",
        "class vision_transformer(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "        H = 32,\n",
        "        W = 32,\n",
        "        embed_dim  = 4,\n",
        "        MLP_size   = 2,\n",
        "        num_class  = 2,\n",
        "        patch_size = 8,\n",
        "        num_head   = 2,\n",
        "        batch_size = 1,\n",
        "        in_channel = 3,\n",
        "        ):\n",
        "\n",
        "        N = int(H*W/(patch_size**2))\n",
        "        super(vision_transformer, self).__init__()\n",
        "        self.preprocess = nn.Sequential(\n",
        "            PatchEmbedding(patch_size, in_channel),\n",
        "            Concatenate_CLS_Token(N, in_channel),\n",
        "            Add_Positional_Embedding(N, in_channel)\n",
        "        )\n",
        "        self.transformer = []\n",
        "        self.mlp = []\n",
        "\n",
        "        self.transformer1 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    MultiHeadAttention(num_head,in_channel,embed_dim)\n",
        "                    )\n",
        "        self.mlp1 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    nn.Linear(in_channel,MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, in_channel)\n",
        "                )\n",
        "        self.transformer2 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    MultiHeadAttention(num_head,in_channel,embed_dim)\n",
        "                    )\n",
        "        self.mlp2 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    nn.Linear(in_channel, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, in_channel)\n",
        "                )\n",
        "        self.transformer3 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    MultiHeadAttention(num_head,in_channel,embed_dim)\n",
        "                    )\n",
        "        self.mlp3 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    nn.Linear(in_channel, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, in_channel)\n",
        "                )\n",
        "        self.transformer4 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    MultiHeadAttention(num_head,in_channel,embed_dim)\n",
        "                    )\n",
        "        self.mlp4 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    nn.Linear(in_channel, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size,in_channel)\n",
        "                )\n",
        "        self.transformer5 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    MultiHeadAttention(num_head,in_channel,embed_dim)\n",
        "                    )\n",
        "        self.mlp5 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    nn.Linear(in_channel, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, in_channel)\n",
        "                )\n",
        "        self.transformer6 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    MultiHeadAttention(num_head,in_channel,embed_dim)\n",
        "                    )\n",
        "        self.mlp6 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    nn.Linear(in_channel, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, in_channel)\n",
        "                )\n",
        "        self.transformer7 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    MultiHeadAttention(num_head,in_channel,embed_dim)\n",
        "                    )\n",
        "        self.mlp7 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    nn.Linear(in_channel, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, in_channel)\n",
        "                )\n",
        "        self.transformer8 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    MultiHeadAttention(num_head,in_channel,embed_dim)\n",
        "                    )\n",
        "        self.mlp8 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    nn.Linear(in_channel, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, in_channel)\n",
        "                )\n",
        "        self.transformer9 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    MultiHeadAttention(num_head,in_channel,embed_dim)\n",
        "                    )\n",
        "        self.mlp9 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    nn.Linear(in_channel, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, in_channel)\n",
        "                )\n",
        "        self.transformer10 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    MultiHeadAttention(num_head,in_channel,embed_dim)\n",
        "                    )\n",
        "        self.mlp10 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    nn.Linear(in_channel, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, in_channel)\n",
        "                )\n",
        "        self.transformer11 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    MultiHeadAttention(num_head,in_channel,embed_dim)\n",
        "                    )\n",
        "        self.mlp11 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    nn.Linear(in_channel, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, in_channel)\n",
        "                )\n",
        "        self.transformer12 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    MultiHeadAttention(num_head,in_channel,embed_dim)\n",
        "                    )\n",
        "        self.mlp12 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, in_channel]),\n",
        "                    nn.Linear(in_channel, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, in_channel)\n",
        "                )\n",
        "\n",
        "        self.head = nn.Linear(in_channel, num_class)\n",
        "    def forward(self, x):\n",
        "        x  = self.preprocess(x)\n",
        "        x1 = self.transformer1(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp1(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer2(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp2(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer3(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp3(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer4(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp4(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer5(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp5(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer6(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp6(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer7(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp7(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer8(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp8(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer9(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp9(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer10(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp10(x)\n",
        "        x  = x+x1\n",
        "\n",
        "\n",
        "        x1 = self.transformer11(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp11(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer12(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp12(x)\n",
        "        x  = x+x1\n",
        "        out    = x[:,0,:]\n",
        "        out    = self.head(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PHTwNm6xS7xx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}