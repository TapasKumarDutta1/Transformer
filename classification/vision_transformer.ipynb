{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwZisEQsTGwC"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # (B, E, H, W)\n",
        "        B,C,H,W = x.shape\n",
        "        x = x.view(B,C,H*W)  # (B, E, H*W)\n",
        "        x = x.transpose(1, 2)  # (B, H*W, E)\n",
        "        return x\n",
        "\n",
        "class Concatenate_CLS_Token(nn.Module):\n",
        "    def __init__(self, N, embed_dim):\n",
        "        super().__init__()\n",
        "        self.CLS = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "    def forward(self, x):\n",
        "        BATCH, _, _ =x.shape\n",
        "        CLS = self.CLS.repeat(BATCH,1,1)\n",
        "        x   = torch.concatenate([CLS, x],1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Add_Positional_Embedding(nn.Module):\n",
        "    def __init__(self, N, embed_dim):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.positional = nn.Parameter(torch.zeros(1, 1, int(embed_dim)))\n",
        "    def forward(self, x):\n",
        "        BATCH, _, _ =x.shape\n",
        "        positional = self.positional.repeat(BATCH,1,1)\n",
        "        x = x+positional\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,head,in_dim):\n",
        "    super().__init__()\n",
        "    self.head = head\n",
        "    self.query_projection = nn.Linear(in_dim, in_dim)\n",
        "    self.key_projection   = nn.Linear(in_dim, in_dim)\n",
        "    self.value_projection = nn.Linear(in_dim, in_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    query = self.query_projection(x)   #B, N, D\n",
        "    key   = self.key_projection(x)\n",
        "    value = self.value_projection(x)\n",
        "\n",
        "\n",
        "    B,N,D = x.shape\n",
        "\n",
        "\n",
        "    query = query.view(B, N, self.head, D//self.head)  # B, N, H, D/H\n",
        "    key   = key.view(B, N, self.head, D//self.head)\n",
        "    value = value.view(B, N, self.head, D//self.head)\n",
        "\n",
        "\n",
        "    query = query.transpose(1,2)  # B, H, N, D/H\n",
        "    key   = key.transpose(1,2)\n",
        "    value = value.transpose(1,2)  # B, H, N, D/H\n",
        "\n",
        "\n",
        "    attention_map = torch.matmul(query, key.transpose(-1,-2))   #B, H, N, N\n",
        "    scaled_attention_map = attention_map / torch.sqrt(torch.tensor((D // self.head)))\n",
        "    scaled_attention_map = torch.nn.Softmax(-1)(scaled_attention_map)\n",
        "\n",
        "\n",
        "    output = torch.matmul(scaled_attention_map, value)   # B, H, N, D/H\n",
        "    output = output.transpose(1,2)\n",
        "    output = output.reshape(B, N, D)\n",
        "    return output\n",
        "\n",
        "class vision_transformer(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "        H = 32,\n",
        "        W = 32,\n",
        "        embed_dim  = 768,\n",
        "        MLP_size   = 3072,\n",
        "        num_class  = 1,\n",
        "        patch_size = 50,\n",
        "        num_head   = 8,\n",
        "        batch_size = 32,\n",
        "        in_channel = 1,\n",
        "        ):\n",
        "\n",
        "        N = int(H*W/(patch_size**2))\n",
        "        super(vision_transformer, self).__init__()\n",
        "        self.preprocess = nn.Sequential(\n",
        "            PatchEmbedding(patch_size, in_channel, embed_dim),\n",
        "            Concatenate_CLS_Token(N, embed_dim),\n",
        "            Add_Positional_Embedding(N, embed_dim)\n",
        "        )\n",
        "        self.transformer = []\n",
        "        self.mlp = []\n",
        "\n",
        "        self.transformer1 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    MultiHeadAttention(num_head,embed_dim)\n",
        "                    )\n",
        "        self.mlp1 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    nn.Linear(embed_dim, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, embed_dim)\n",
        "                )\n",
        "        self.transformer2 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    MultiHeadAttention(num_head,embed_dim)\n",
        "                    )\n",
        "        self.mlp2 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    nn.Linear(embed_dim, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, embed_dim)\n",
        "                )\n",
        "        self.transformer3 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    MultiHeadAttention(num_head,embed_dim)\n",
        "                    )\n",
        "        self.mlp3 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    nn.Linear(embed_dim, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, embed_dim)\n",
        "                )\n",
        "        self.transformer4 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    MultiHeadAttention(num_head,embed_dim)\n",
        "                    )\n",
        "        self.mlp4 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    nn.Linear(embed_dim, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, embed_dim)\n",
        "                )\n",
        "        self.transformer5 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    MultiHeadAttention(num_head,embed_dim)\n",
        "                    )\n",
        "        self.mlp5 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    nn.Linear(embed_dim, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, embed_dim)\n",
        "                )\n",
        "        self.transformer6 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    MultiHeadAttention(num_head,embed_dim)\n",
        "                    )\n",
        "        self.mlp6 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    nn.Linear(embed_dim, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, embed_dim)\n",
        "                )\n",
        "        self.transformer7 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    MultiHeadAttention(num_head,embed_dim)\n",
        "                    )\n",
        "        self.mlp7 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    nn.Linear(embed_dim, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, embed_dim)\n",
        "                )\n",
        "        self.transformer8 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    MultiHeadAttention(num_head,embed_dim)\n",
        "                    )\n",
        "        self.mlp8 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    nn.Linear(embed_dim, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, embed_dim)\n",
        "                )\n",
        "        self.transformer9 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    MultiHeadAttention(num_head,embed_dim)\n",
        "                    )\n",
        "        self.mlp9 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    nn.Linear(embed_dim, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, embed_dim)\n",
        "                )\n",
        "        self.transformer10 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    MultiHeadAttention(num_head,embed_dim)\n",
        "                    )\n",
        "        self.mlp10 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    nn.Linear(embed_dim, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, embed_dim)\n",
        "                )\n",
        "        self.transformer11 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    MultiHeadAttention(num_head,embed_dim)\n",
        "                    )\n",
        "        self.mlp11 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    nn.Linear(embed_dim, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, embed_dim)\n",
        "                )\n",
        "        self.transformer12 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    MultiHeadAttention(num_head,embed_dim)\n",
        "                    )\n",
        "        self.mlp12 = nn.Sequential(\n",
        "                    nn.LayerNorm([N+1, embed_dim]),\n",
        "                    nn.Linear(embed_dim, MLP_size),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(MLP_size, embed_dim)\n",
        "                )\n",
        "\n",
        "        self.head = nn.Linear(embed_dim, num_class)\n",
        "    def forward(self, x):\n",
        "        x  = self.preprocess(x)\n",
        "\n",
        "        x1 = self.transformer1(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp1(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer2(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp2(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer3(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp3(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer4(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp4(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer5(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp5(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer6(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp6(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer7(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp7(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer8(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp8(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer9(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp9(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer10(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp10(x)\n",
        "        x  = x+x1\n",
        "\n",
        "\n",
        "        x1 = self.transformer11(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp11(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.transformer12(x)\n",
        "        x  = x+x1\n",
        "\n",
        "        x1 = self.mlp12(x)\n",
        "        x  = x+x1\n",
        "        out    = x[:,0,:]\n",
        "        out    = self.head(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUWFxwW7d2fx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
